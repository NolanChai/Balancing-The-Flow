{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f004d50-720b-4087-a710-d45696d2697e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04a481a-4b26-4291-bd52-51a6ee5fbb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# for weird env bug\n",
    "current_dir = os.getcwd()\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "    \n",
    "# Now import from utils\n",
    "from utils import *\n",
    "from utils import LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8356713f-5e69-4ef3-8704-70b45c068f40",
   "metadata": {},
   "source": [
    "### Huggingface Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e330a1ee-5a61-4143-8da2-5763081cef2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cdd431e1e54470b6f1cea88ed11ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf28a3f-7051-4bbc-9743-453e3f6cf2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-14 15:02:46] Successfully connected to API at http://localhost:1234/v1.\n",
      "[2025-05-14 15:02:46] Loading cached dataset abisee_cnn_dailymail ✓ Done   \n",
      "[2025-05-14 15:02:46] Loaded dataset abisee_cnn_dailymail from cache with 11490 examples\n",
      "                                                                           "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6554aceb5ff34a398ab10b8d85ea0d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[2025-05-14 15:02:46] Batch generation complete: 0 generated, 0 retries, 0 errors\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[2025-05-14 15:02:46] Batch generation complete: 0 generated, 0 retries, 0 errors\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-14 15:02:46] Resuming from ID 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = LLM.Model(model=\"llama-2-7b\")\n",
    "test.batch_generate(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9a0ccc-73b5-4397-97d8-b8f13becf2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2025-05-14 15:02:51]] Finding all generation files ✓ Done   \n",
      "                                                              "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccf09c5a0664277a88946a228274bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-14 15:02:53] Consolidated 4 datasets into CSV files in /Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'daily_dialog': PosixPath('/Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/daily_dialog_consolidated.csv'),\n",
       " 'cnn_dailymail': PosixPath('/Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/cnn_dailymail_consolidated.csv'),\n",
       " 'WildChat': PosixPath('/Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/WildChat_consolidated.csv'),\n",
       " 'writingprompts': PosixPath('/Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/writingprompts_consolidated.csv')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.utils import consolidate_generations\n",
    "\n",
    "consolidate_generations(\n",
    "    r\"/Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations\",\n",
    "    r\"/Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6fe117-02f7-4667-a014-546ab247eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import calculate_surprisals, process_csv_files_with_surprisals\n",
    "from utils.utils import analyze_token_lengths, plot_token_length_distribution, save_token_stats_to_csv, calculate_long_text_surprisals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153e8e5f-ef5a-4612-9055-3718f9085748",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\n",
    "        \"/Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/cnn_dailymail_consolidated.csv\",\n",
    "        \"/Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/daily_dialog_consolidated.csv\",\n",
    "        \"/Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/WildChat_consolidated.csv\",\n",
    "        \"/Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/writingprompts_consolidated.csv\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe615ac-1c23-4199-a0fd-2edf1c61f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing /Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/cnn_dailymail_consolidated.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1637 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for /Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/cnn_dailymail_consolidated.csv:\n",
      "  - Total texts: 2400\n",
      "  - Texts exceeding 1024 tokens: 476 (19.83%)\n",
      "  - Token count statistics:\n",
      "    - Min: 2\n",
      "    - Mean: 569.09\n",
      "    - Median: 300\n",
      "    - Max: 2185\n",
      "    - 90th percentile: 1784.10\n",
      "    - 95th percentile: 1831.05\n",
      "    - 99th percentile: 1927.01\n",
      "  - Distribution by token count range:\n",
      "    - 0-256: 1029 texts (42.88%)\n",
      "    - 257-512: 663 texts (27.62%)\n",
      "    - 513-768: 166 texts (6.92%)\n",
      "    - 769-1024: 66 texts (2.75%)\n",
      "    - 1025-1536: 46 texts (1.92%)\n",
      "    - 1537-2048: 424 texts (17.67%)\n",
      "    - 2048+: 6 texts (0.25%)\n",
      "Analyzing /Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/daily_dialog_consolidated.csv...\n",
      "Results for /Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/daily_dialog_consolidated.csv:\n",
      "  - Total texts: 2400\n",
      "  - Texts exceeding 1024 tokens: 137 (5.71%)\n",
      "  - Token count statistics:\n",
      "    - Min: 0\n",
      "    - Mean: 274.90\n",
      "    - Median: 99\n",
      "    - Max: 6915\n",
      "    - 90th percentile: 644.10\n",
      "    - 95th percentile: 1173.70\n",
      "    - 99th percentile: 2434.71\n",
      "  - Distribution by token count range:\n",
      "    - 0-256: 1759 texts (73.29%)\n",
      "    - 257-512: 301 texts (12.54%)\n",
      "    - 513-768: 101 texts (4.21%)\n",
      "    - 769-1024: 68 texts (2.83%)\n",
      "    - 1025-1536: 41 texts (1.71%)\n",
      "    - 1537-2048: 54 texts (2.25%)\n",
      "    - 2048+: 42 texts (1.75%)\n",
      "Analyzing /Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/WildChat_consolidated.csv...\n",
      "Results for /Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/WildChat_consolidated.csv:\n",
      "  - Total texts: 2400\n",
      "  - Texts exceeding 1024 tokens: 368 (15.33%)\n",
      "  - Token count statistics:\n",
      "    - Min: 0\n",
      "    - Mean: 507.41\n",
      "    - Median: 153\n",
      "    - Max: 7917\n",
      "    - 90th percentile: 1578.20\n",
      "    - 95th percentile: 2227.85\n",
      "    - 99th percentile: 4241.82\n",
      "  - Distribution by token count range:\n",
      "    - 0-256: 522 texts (21.75%)\n",
      "    - 257-512: 355 texts (14.79%)\n",
      "    - 513-768: 184 texts (7.67%)\n",
      "    - 769-1024: 116 texts (4.83%)\n",
      "    - 1025-1536: 124 texts (5.17%)\n",
      "    - 1537-2048: 102 texts (4.25%)\n",
      "    - 2048+: 142 texts (5.92%)\n",
      "Analyzing /Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/writingprompts_consolidated.csv...\n",
      "Results for /Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/writingprompts_consolidated.csv:\n",
      "  - Total texts: 2353\n",
      "  - Texts exceeding 1024 tokens: 580 (24.65%)\n",
      "  - Token count statistics:\n",
      "    - Min: 0\n",
      "    - Mean: 666.42\n",
      "    - Median: 368\n",
      "    - Max: 3440\n",
      "    - 90th percentile: 1893.00\n",
      "    - 95th percentile: 1934.00\n",
      "    - 99th percentile: 2028.48\n",
      "  - Distribution by token count range:\n",
      "    - 0-256: 907 texts (38.55%)\n",
      "    - 257-512: 526 texts (22.35%)\n",
      "    - 513-768: 229 texts (9.73%)\n",
      "    - 769-1024: 107 texts (4.55%)\n",
      "    - 1025-1536: 108 texts (4.59%)\n",
      "    - 1537-2048: 454 texts (19.29%)\n",
      "    - 2048+: 18 texts (0.76%)\n",
      "Saved token analysis summary to token_analysis/token_analysis_summary.csv\n"
     ]
    }
   ],
   "source": [
    "analysis_results = analyze_token_lengths(\n",
    "        csv_files=csv_files,\n",
    "        tokenizer_name=\"gpt2\",\n",
    "        threshold=1024\n",
    "    )\n",
    "plot_token_length_distribution(\n",
    "        analysis_results=analysis_results,\n",
    "        output_dir=\"token_analysis_plots\"\n",
    "    )\n",
    "save_token_stats_to_csv(\n",
    "        analysis_results=analysis_results,\n",
    "        output_dir=\"token_analysis\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba35a36-d865-4457-9f2f-e6c3618605dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/nolan/Documents/GitHub/Balancing-The-Flow/Generations-csv/cnn_dailymail_consolidated.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mprocess_csv_files_with_surprisals\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcsv_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcsv_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_source\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msurprisal_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/utils/utils.py:451\u001b[39m, in \u001b[36mprocess_csv_files_with_surprisals\u001b[39m\u001b[34m(csv_files, tokenizer_source, max_length, output_dir)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    442\u001b[39m \u001b[33;03mProcess CSV files and calculate surprisals, saving results to CSV files.\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    448\u001b[39m \u001b[33;03m    output_dir: Directory to save output CSV files\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# Calculate surprisals\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m results = \u001b[43mcalculate_surprisals\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcsv_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcsv_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_source\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[38;5;66;03m# Print summary statistics\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file, data \u001b[38;5;129;01min\u001b[39;00m results.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/utils/utils.py:364\u001b[39m, in \u001b[36mcalculate_surprisals\u001b[39m\u001b[34m(csv_files, tokenizer_source, max_length, output_dir)\u001b[39m\n\u001b[32m    360\u001b[39m tokens = tokenizer.convert_ids_to_tokens(token_ids[\u001b[32m0\u001b[39m])\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    363\u001b[39m     \u001b[38;5;66;03m# Get model output\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m     logits = outputs.logits\n\u001b[32m    367\u001b[39m     \u001b[38;5;66;03m# Shift logits and tokens for calculating next token prediction\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/.venv/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1079\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/.venv/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:820\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    819\u001b[39m     inputs_embeds = \u001b[38;5;28mself\u001b[39m.wte(input_ids)\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m position_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    821\u001b[39m hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n\u001b[32m    823\u001b[39m \u001b[38;5;66;03m# Attention mask.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Balancing-The-Flow/revised-scripts/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "results = process_csv_files_with_surprisals(\n",
    "    csv_files=csv_files,\n",
    "    tokenizer_source=\"gpt2\",\n",
    "    max_length=2048,\n",
    "    output_dir=\"surprisal_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9052de1-546d-4d89-ac42-cf268ef9f291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

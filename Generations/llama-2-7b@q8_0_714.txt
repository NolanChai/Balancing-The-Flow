
    C:\Users\Nick\AppData\Local\Temp\anaconda3_2019-06-06T12_45_47Z_1\envs\py38\lib\site-packages\sklearn\model_selection\_search.py:262: UserWarning:
      The search algorithm was unable to find any suitable scoring methods for this combination of parameters,
      please increase the number of allowed combinations and try again.
    warnings.warn("The search algorithm was unable to find any suitable scoring methods for this combination of parameters, please increase the number of allowed combinations and try again.")

### Solution:

This is a case of an "unable to find any suitable scoring methods" warning message. I tried adding more iterations but it didn't solve my issue either. The issue was solved by setting `cv=0`. Here is the complete code with my solution (line 24):

```python
#!/usr/bin/env python3
import os, random, sys
sys.path.insert(0, '.')
from utils import *
import pickle

def cross_validation_grid_search_hyperparameters():
    # The scoring metrics available for the GridSearchCV are as follows:
    # precision = f1_score (recall = True)
    # recall = f1_score (precision = True)
    # accuracy
    # macro avg precision
    # macro avg recall
    # macro avg F1 score

    # The scoring parameters available for the GridSearchCV are as follows:
    # classifier
    # metric
    # param_distributions (for all parameters of a given type)

    # Set these to None if you don't want to specify a parameter distribution.
    # Use -1 or -2 for a uniform distribution over the range [low, high], e.g.,
    # -1.5 would be uniform in the range [-1.5, 1.5].
    cv = None
    scoring_metric = 'accuracy'
    scoring_parameters = dict()
    param_distributions = {}

    if len(sys.argv) == 2:
        print('The input file should be of the form:')
        print('- name: description of a hyperparameter (string), default = None')
        print('- low: lower limit of parameter range (float), default = -1.0')
        print('- high: higher limit of parameter range (float), default = -1.0')
        print('- distr: distribution to be used for uniform sampling (int or tuple, default = None)')

    with open(sys.argv[1], 'rb') as pickle_file:
        pickled_parameters = pickle.load(pickle_file)

    # Iterate over each parameter description in the input file.
    for name in sorted(pickled_parameters):
        if ':' not in name:
            raise ValueError('The parameter description should be of the form <name>: <description> (e.g., "lambda": "learning rate").')

        # Split the parameter name and its description from each other by a colon.
        param_name, _ = name.split(':', 1)

        # If this is not a valid hyperparameter name then skip it.
        if not re.match(r'^[a-zA-Z_\.]+$', param_name):
            continue

        # Iterate over all parameters of the same type and split the description from their names by commas.
        parameter_descriptions = pickled_parameters[param_name].split(',')

        for i in range(len(parameter_descriptions)):
            if ':' not in parameter_descriptions[i]:
                raise ValueError('The parameter description should be of the form <name>: <description> (e.g., "lambda": "learning rate").')

            # Split the name and its description from each other by a colon.
            param_description, _ = parameter_descriptions[i].split(':', 1)

            # If this is not a valid hyperparameter name then skip it.
            if not re.match(r'^[a-zA-Z_\.]+$', param_description):
                continue

            # Add the parameter to be used for grid search.
            if ':' in param_name:
                scoring_parameters[param_name] = param_name, [random.uniform(-1.0, -1.0) if cv == None else cv, random.uniform(pickled_parameters[param_name][-1], pickled_parameters[param_name][0]) if cv != None and len(scoring_parameters) > 0 else None]
            elif cv == None:
                scoring_parameters[param_name] = param_name, [random.uniform(-1.0, -1.0)]
            else:
                raise ValueError('The parameter should be of the form <parameter name>: <lower limit>,<upper limit> (e.g., "lambda": [-1.5, 1.5]).')

    # The GridSearchCV algorithm will only work with a single scoring metric and set of scoring parameters.
    if len(scoring_parameters) > 0:
        scoring_metric = 'accuracy'

    # Specify the search space for hyperparameter grid search.
    search_space = dict()
    search_space[('classifier',)] = ['svm']
    search_space['classifier']['name'] = ['linear','poly']

    # Fit the classifier in cross-validation mode with the specified scoring metric and set of parameters.
    grid_search = GridSearchCV(LinearSVC(), param_distributions=scoring_parameters, scoring=scoring_metric, verbose=0)
    grid_search.fit(X_train, Y_train)
```

```
import pandas as pd
import numpy as np
import re
from nltk import FreqDist
import gensim

df = pd.read_csv('mass_grave_of_mutilated_bodies.txt', sep='\t')

# Create a list of the titles for each news article in the data frame
titles = df['title'].tolist()

# Create a list of the text content for each news article in the data frame
content = df['text'].tolist()

# Combine the title and content into a single string for each news article
combined = []
for title, content in zip(titles, content):
    combined.append(title + ' ' + content)

# Extract all words from the combined list of strings
words = [word for doc in combined for word in re.split(r'\W+', doc)]

# Count the frequency of each word and create a FreqDist object
fdist = FreqDist(words)

# Create a corpus using gensim's Corpus class
corpus = gensim.corpora.Dictionary(words)

# Convert the corpus to a vector space model using TF-IDF weighting
tfidf_model = gensim.models.TfidfModel([corpus])

# Transform the corpus into a matrix of word vectors
X = tfidf_model[corpus]

# Standardize the feature values to have zero mean and unit variance
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_X = scaler.fit_transform(X)

# Create a data frame with the scaled word vectors as features
df_scaled = pd.DataFrame(scaled_X, columns=corpus.keys())
```
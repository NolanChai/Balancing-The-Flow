{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tianq\\Desktop\\LIGN 169\\Balancing-The-Flow\\Scripts\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tianq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA available: True\n",
      "CUDA version: 12.6\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 50257])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"The cat sat on the mat\"\n",
    "input_ids = tokenizer.encode(input, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "probs = torch.softmax(outputs.logits, dim=-1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tianq\\AppData\\Local\\Temp\\ipykernel_5420\\23246384.py:1: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  -1 * np.log2(probs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[12.5820, 10.7411, 15.1673,  ..., 18.6831, 19.9233, 10.6083],\n",
       "         [10.1866, 10.9002, 14.2638,  ..., 22.6555, 14.3161, 13.6603],\n",
       "         [14.6027, 16.1091, 26.5289,  ..., 30.9486, 18.7936, 20.2484],\n",
       "         [20.5522, 18.6787, 27.6044,  ..., 29.2884, 26.3446, 21.9959],\n",
       "         [23.1821, 19.7404, 25.8828,  ..., 25.2227, 25.4650, 22.8360],\n",
       "         [11.7230, 13.1833, 21.3001,  ..., 30.8406, 20.2315, 15.8648]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 * np.log2(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>surprisal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>13.266438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sat</td>\n",
       "      <td>10.472860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on</td>\n",
       "      <td>2.158653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>1.140176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mat</td>\n",
       "      <td>7.949978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token  surprisal\n",
       "0   cat  13.266438\n",
       "1   sat  10.472860\n",
       "2    on   2.158653\n",
       "3   the   1.140176\n",
       "4   mat   7.949978"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(model)\n",
    "batch = to_tokens_and_logprobs(model, tokenizer, [\"The cat sat on the mat\", \"The cat sat on the\"])\n",
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens_and_logprobs(model, tokenizer, input_texts):\n",
    "    # move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model.to(device)\n",
    "\n",
    "    input_ids = tokenizer(input_texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids#.to(device)\n",
    "    outputs = model(input_ids)\n",
    "    probs = torch.softmax(outputs.logits, dim=-1).detach()\n",
    "    surprisals = -1 * np.log2(probs)\n",
    "\n",
    "    # collect the probability of the generated token -- probability at index 0 corresponds to the token at index 1\n",
    "    # input_ids.cpu().detach()\n",
    "    surprisals = surprisals[:, :-1, :]\n",
    "    input_ids = input_ids[:, 1:]\n",
    "    gen_surprisals = torch.gather(surprisals, 2, input_ids[:, :, None]).squeeze(-1)\n",
    "\n",
    "    # gather all the surprisals for the sequences into a neat table\n",
    "    batch = []\n",
    "    sentence_id = 0\n",
    "    for input_sentence, input_surprisals in zip(input_ids, gen_surprisals):\n",
    "        sentence = []\n",
    "        for token, p in zip(input_sentence, input_surprisals):\n",
    "            if token not in tokenizer.all_special_ids:\n",
    "                sentence.append({\n",
    "                    # \"sentence_id\": sentence_id,\n",
    "                    \"token\": tokenizer.decode(token),\n",
    "                    \"surprisal\": p.item()\n",
    "                })\n",
    "        batch.append(pd.DataFrame(sentence))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tianq\\AppData\\Local\\Temp\\ipykernel_9728\\2901210705.py:9: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  surprisals = -1 * np.log2(probs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>surprisal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>5.118950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teenage</td>\n",
       "      <td>14.721128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actor</td>\n",
       "      <td>9.354509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>4.089015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>set</td>\n",
       "      <td>6.280989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>the</td>\n",
       "      <td>0.316361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>next</td>\n",
       "      <td>0.164720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>five</td>\n",
       "      <td>3.079309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>years</td>\n",
       "      <td>0.046524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>.</td>\n",
       "      <td>0.461701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        token  surprisal\n",
       "0         The   5.118950\n",
       "1     teenage  14.721128\n",
       "2       actor   9.354509\n",
       "3          is   4.089015\n",
       "4         set   6.280989\n",
       "..        ...        ...\n",
       "214       the   0.316361\n",
       "215      next   0.164720\n",
       "216      five   3.079309\n",
       "217     years   0.046524\n",
       "218         .   0.461701\n",
       "\n",
       "[219 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(\"../Generations\")\n",
    "texts=[]\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "for filepath in files[:10]:\n",
    "    with open(\"../Generations/\" + filepath, 'r') as file:\n",
    "        text = file.read()\n",
    "    texts.append(text)\n",
    "\n",
    "batch = to_tokens_and_logprobs(model, tokenizer, texts)\n",
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(219, 2)\n",
      "(839, 2)\n",
      "(1023, 2)\n",
      "(486, 2)\n",
      "(157, 2)\n",
      "(496, 2)\n",
      "(647, 2)\n",
      "(561, 2)\n",
      "(174, 2)\n",
      "(222, 2)\n"
     ]
    }
   ],
   "source": [
    "for text in batch:\n",
    "    print(text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.642322754064937 34.29004496096813\n",
      "7.86144779533585 15.725233124652565\n",
      "9.443031910349875 19.42506240869066\n",
      "11.484577992179283 23.46479639157494\n",
      "12.4728244985312 23.10573174920876\n",
      "12.487591018076383 22.9211969608712\n",
      "10.594795901538907 21.91790469405356\n",
      "8.55277134133881 17.235762894157862\n",
      "16.409818469373388 35.68560934315286\n",
      "13.219312761842966 25.676247881393657\n"
     ]
    }
   ],
   "source": [
    "def UID_variance(text):\n",
    "    N = text.shape[0]\n",
    "    mu = text['surprisal'].mean()\n",
    "    surprisals = text['surprisal']\n",
    "    return ((surprisals - mu) ** 2).sum() / N\n",
    "\n",
    "def UID_pairwise(text):\n",
    "    N = text.shape[0]\n",
    "    surprisals = text['surprisal']\n",
    "    return (surprisals.diff() ** 2).sum() / (N - 1)\n",
    "for text in batch:\n",
    "    print(UID_variance(text), UID_pairwise(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
